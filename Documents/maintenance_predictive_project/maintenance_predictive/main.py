"""
MAIN.PY - ORCHESTRATION COMPL√àTE DU PIPELINE
Ex√©cute toutes les √©tapes du projet de A √† Z
"""

import sys
import logging
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Imports locaux
from src.utils import (
    creer_dossiers, generer_donnees_capteurs, 
    tracer_series_temporelles, tracer_distributions_pannes, 
    tracer_correlation_heatmap, afficher_info_data
)
from src.preprocessing import DataPreprocessor
from src.feature_engineering import FeatureEngineer
from src.train_model import ModelTrainer
from src.evaluate import ModelEvaluator

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    'n_machines': 5,
    'samples_par_machine': 2000,
    'test_size': 0.2,
    'random_state': 42,
    'seq_length': 30,  # Pour LSTM
    'epochs': 50,      # Pour LSTM
}

# ============================================================================
# PIPELINE PRINCIPAL
# ============================================================================

def pipeline_complet():
    """Ex√©cute le pipeline complet du d√©but √† la fin"""
    
    logger.info("‚ïî" + "=" * 78 + "‚ïó")
    logger.info("‚ïë" + " MAINTENANCE PR√âDICTIVE - PIPELINE COMPLET ".center(78) + "‚ïë")
    logger.info("‚ïö" + "=" * 78 + "‚ïù\n")
    
    # ========== √âTAPE 0 : PR√âPARATION ==========
    logger.info("\nüîß √âTAPE 0 : PR√âPARATION")
    logger.info("‚îÄ" * 80)
    creer_dossiers()
    
    # ========== √âTAPE 1 : G√âN√âRATION DONN√âES ==========
    logger.info("\nüìä √âTAPE 1 : G√âN√âRATION DONN√âES IoT")
    logger.info("‚îÄ" * 80)
    df_brut = generer_donnees_capteurs(
        n_machines=CONFIG['n_machines'],
        samples_par_machine=CONFIG['samples_par_machine']
    )
    
    # Sauvegarder donn√©es brutes
    df_brut.to_csv('data/raw/donnees_capteurs.csv', index=False)
    logger.info("‚úì Donn√©es brutes sauvegard√©es : data/raw/donnees_capteurs.csv")
    
    # Afficher infos
    afficher_info_data(df_brut)
    
    # ========== √âTAPE 2 : EDA (ANALYSE EXPLORATOIRE) ==========
    logger.info("\nüìà √âTAPE 2 : ANALYSE EXPLORATOIRE DES DONN√âES (EDA)")
    logger.info("‚îÄ" * 80)
    
    tracer_series_temporelles(df_brut, machine_id=0)
    tracer_distributions_pannes(df_brut)
    tracer_correlation_heatmap(df_brut)
    
    logger.info("‚úì Visualisations cr√©√©es et sauvegard√©es dans results/")
    
    # ========== √âTAPE 3 : PR√âTRAITEMENT ==========
    logger.info("\nüßπ √âTAPE 3 : PR√âTRAITEMENT DES DONN√âES")
    logger.info("‚îÄ" * 80)
    
    preprocessor = DataPreprocessor(scaler_type='standard')
    
    # Sauvegarder donn√©es brutes d'abord
    df_brut.to_csv('data/raw/donnees_capteurs.csv', index=False)
    
    # Pr√©traiter
    df_processed = preprocessor.pipeline_complet(
        chemin_csv='data/raw/donnees_capteurs.csv',
        colonnes_features=['temperature', 'vibration', 'pression', 'courant'],
        normaliser=True,
        detecter_anomalies_flag=True
    )
    
    # Sauvegarder
    df_processed.to_csv('data/processed/donnees_preprocessees.csv', index=False)
    logger.info("‚úì Donn√©es pr√©trait√©es sauvegard√©es : data/processed/donnees_preprocessees.csv")
    
    # ========== √âTAPE 4 : FEATURE ENGINEERING ==========
    logger.info("\n‚öôÔ∏è  √âTAPE 4 : FEATURE ENGINEERING")
    logger.info("‚îÄ" * 80)
    
    engineer = FeatureEngineer(window_size=10)
    df_features = engineer.pipeline_features(
        df_processed,
        colonnes_capteurs=['temperature', 'vibration', 'pression', 'courant']
    )
    
    # Sauvegarder
    df_features.to_csv('data/processed/donnees_features.csv', index=False)
    logger.info("‚úì Features sauvegard√©es : data/processed/donnees_features.csv")
    
    # ========== √âTAPE 5 : PR√âPARATION DONN√âES POUR MOD√àLES ==========
    logger.info("\nüéØ √âTAPE 5 : PR√âPARATION DONN√âES POUR MOD√àLES")
    logger.info("‚îÄ" * 80)
    
    # S√©lectionner colonnes features (tout sauf timestamp, machine_id, panne)
    colonnes_features = [c for c in df_features.columns 
                        if c not in ['timestamp', 'machine_id', 'panne', 'anomalie']]
    
    X = df_features[colonnes_features].values
    y = df_features['panne'].values
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=CONFIG['test_size'],
        random_state=CONFIG['random_state'],
        stratify=y
    )
    
    logger.info(f"‚úì Data split :")
    logger.info(f"  Train : {X_train.shape[0]} samples")
    logger.info(f"  Test  : {X_test.shape[0]} samples")
    logger.info(f"  Features : {X_train.shape[1]}")
    
    # ========== √âTAPE 6 : CR√âATION S√âQUENCES LSTM ==========
    logger.info("\nüîÑ √âTAPE 6 : CR√âATION S√âQUENCES POUR LSTM")
    logger.info("‚îÄ" * 80)
    
    # R√©appliquer feature engineer sur s√©quences
    X_train_seq, y_train_seq = engineer.creer_sequences_lstm(
        df_features.iloc[:len(X_train)],
        seq_length=CONFIG['seq_length'],
        colonnes_features=colonnes_features
    )
    
    X_test_seq, y_test_seq = engineer.creer_sequences_lstm(
        df_features.iloc[len(X_train):],
        seq_length=CONFIG['seq_length'],
        colonnes_features=colonnes_features
    )
    
    # ========== √âTAPE 7 : ENTRA√éNEMENT MOD√àLES ==========
    logger.info("\nüöÄ √âTAPE 7 : ENTRA√éNEMENT DES MOD√àLES")
    logger.info("‚îÄ" * 80)
    
    trainer = ModelTrainer(random_state=CONFIG['random_state'])
    
    # Random Forest
    model_rf, train_score_rf, test_score_rf = trainer.train_random_forest(
        X_train, y_train, X_test, y_test
    )
    trainer.sauvegarder_model('random_forest')
    
    # XGBoost
    model_xgb, train_score_xgb, test_score_xgb = trainer.train_xgboost(
        X_train, y_train, X_test, y_test
    )
    trainer.sauvegarder_model('xgboost')
    
    # LSTM
    model_lstm, history = trainer.train_lstm(
        X_train_seq, y_train_seq, 
        X_test_seq, y_test_seq,
        epochs=CONFIG['epochs'],
        batch_size=32
    )
    trainer.sauvegarder_model('lstm')
    
    # ========== √âTAPE 8 : √âVALUATION MOD√àLES ==========
    logger.info("\nüìã √âTAPE 8 : √âVALUATION DES MOD√àLES")
    logger.info("‚îÄ" * 80)
    
    evaluator = ModelEvaluator()
    
    # √âvaluer chaque mod√®le
    evaluator.evaluer_classification(model_rf, X_test, y_test, 'Random Forest')
    evaluator.evaluer_classification(model_xgb, X_test, y_test, 'XGBoost')
    
    # Pour LSTM, ajuster les donn√©es
    eval_lstm_loss, eval_lstm_acc, eval_lstm_auc = model_lstm.evaluate(
        X_test_seq, y_test_seq, verbose=0
    )
    logger.info(f"\nüß† LSTM - R√©sultats :")
    logger.info(f"  Accuracy : {eval_lstm_acc:.4f}")
    logger.info(f"  AUC      : {eval_lstm_auc:.4f}")
    
    # Comparaison
    df_comparaison = evaluator.comparer_modeles()
    
    # Visualisations
    evaluator.tracer_confusion_matrix('Random Forest')
    evaluator.tracer_confusion_matrix('XGBoost')
    evaluator.tracer_comparaison()
    
    # Rapport complet
    evaluator.generer_rapport()
    
    # ========== √âTAPE 9 : R√âSUM√â FINAL ==========
    logger.info("\n" + "=" * 80)
    logger.info("‚úì PIPELINE TERMIN√â AVEC SUCC√àS!")
    logger.info("=" * 80)
    
    logger.info("\nüìÅ Fichiers g√©n√©r√©s :")
    logger.info("  Data :")
    logger.info("    - data/raw/donnees_capteurs.csv")
    logger.info("    - data/processed/donnees_preprocessees.csv")
    logger.info("    - data/processed/donnees_features.csv")
    logger.info("  Mod√®les :")
    logger.info("    - models/random_forest_model.pkl")
    logger.info("    - models/xgboost_model.pkl")
    logger.info("    - models/lstm_model.h5")
    logger.info("  R√©sultats :")
    logger.info("    - results/*.png (visualisations)")
    logger.info("    - results/rapport_evaluation.txt")
    
    logger.info("\nüöÄ Prochaines √©tapes :")
    logger.info("  1. Lancer le dashboard : streamlit run dashboard/app.py")
    logger.info("  2. Explorer les r√©sultats dans results/")
    logger.info("  3. Affiner les mod√®les avec de nouvelles donn√©es")

if __name__ == "__main__":
    try:
        pipeline_complet()
    except Exception as e:
        logger.error(f"‚úó ERREUR : {e}", exc_info=True)
        sys.exit(1)